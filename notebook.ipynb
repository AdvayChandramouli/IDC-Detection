{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# image processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# plotting (we will need that later for our plotes and all)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning models (we want to try a lot of them and see which one performs the best!)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# balancing the dataset (we need that later too)\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory paths\n",
    "data_dir = '/Users/josuegodeme/Downloads/AI4All Project/archive'  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Initialize lists to hold file paths and labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "# Walk through the directory and collect image paths and labels\n",
    "for dirname, _, filenames in os.walk(data_dir):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.png'):\n",
    "            # Full path to the image\n",
    "            file_path = os.path.join(dirname, filename)\n",
    "            image_paths.append(file_path)\n",
    "            \n",
    "            # Extract the class label from the filename\n",
    "            if 'class0' in filename:\n",
    "                labels.append(0)\n",
    "            elif 'class1' in filename:\n",
    "                labels.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 277524\n",
      "Total labels: 277524\n",
      "Class distribution: Counter({0: 198738, 1: 78786})\n"
     ]
    }
   ],
   "source": [
    "print(f'Total images: {len(image_paths)}')\n",
    "print(f'Total labels: {len(labels)}')\n",
    "\n",
    "# Check the distribution of classes\n",
    "from collections import Counter\n",
    "print('Class distribution:', Counter(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to hold image data\n",
    "images = []\n",
    "\n",
    "# Loop over image paths and load images\n",
    "for path in image_paths:\n",
    "    # Load the image\n",
    "    img = cv2.imread(path)\n",
    "    \n",
    "    # Convert BGR to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Normalize the image\n",
    "    img = img / 255.0\n",
    "    \n",
    "    # Append to the images list\n",
    "    images.append(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "import sys\n",
    "\n",
    "def debug_image_data(images: List) -> None:\n",
    "    \"\"\"\n",
    "    Debug image data structure and content.\n",
    "    \n",
    "    Args:\n",
    "        images: List of images to analyze\n",
    "    \"\"\"\n",
    "    print(\"=== Data Structure Analysis ===\")\n",
    "    print(f\"Type of images container: {type(images)}\")\n",
    "    print(f\"Length of images container: {len(images)}\")\n",
    "    \n",
    "    # Check first few images\n",
    "    for i in range(min(5, len(images))):\n",
    "        print(f\"\\nImage {i}:\")\n",
    "        print(f\"  Type: {type(images[i])}\")\n",
    "        try:\n",
    "            print(f\"  Shape: {np.array(images[i]).shape}\")\n",
    "            print(f\"  Data type: {np.array(images[i]).dtype}\")\n",
    "            print(f\"  Min value: {np.min(images[i])}\")\n",
    "            print(f\"  Max value: {np.max(images[i])}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error converting to array: {str(e)}\")\n",
    "            \n",
    "        # Memory size\n",
    "        try:\n",
    "            print(f\"  Approximate memory size: {sys.getsizeof(images[i]) / 1024:.2f} KB\")\n",
    "        except:\n",
    "            print(\"  Could not determine memory size\")\n",
    "\n",
    "def convert_images_safely(images: List) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Safely convert images to numpy array with additional error checking.\n",
    "    \n",
    "    Args:\n",
    "        images: List of images\n",
    "        \n",
    "    Returns:\n",
    "        Numpy array of images\n",
    "    \"\"\"\n",
    "    # First, verify each image can be converted individually\n",
    "    processed_images = []\n",
    "    for i, img in enumerate(images):\n",
    "        try:\n",
    "            img_array = np.asarray(img)\n",
    "            if img_array.shape != (50, 50, 3):\n",
    "                raise ValueError(f\"Image {i} has unexpected shape: {img_array.shape}\")\n",
    "            processed_images.append(img_array)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {i}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to final array\n",
    "    if processed_images:\n",
    "        return np.stack(processed_images)\n",
    "    else:\n",
    "        raise ValueError(\"No images could be processed successfully\")\n",
    "\n",
    "# Example usage:\n",
    "def process_image_data(images):\n",
    "    \"\"\"\n",
    "    Process image data with detailed debugging.\n",
    "    \"\"\"\n",
    "    print(\"Analyzing image data structure...\")\n",
    "    debug_image_data(images)\n",
    "    \n",
    "    print(\"\\nAttempting safe conversion...\")\n",
    "    try:\n",
    "        X = convert_images_safely(images)\n",
    "        print(f\"Success! Final array shape: {X.shape}\")\n",
    "        return X\n",
    "    except Exception as e:\n",
    "        print(f\"Conversion failed: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from PIL import Image\n",
    "\n",
    "def resize_and_normalize_images(images: List[np.ndarray], \n",
    "                              target_size: Tuple[int, int] = (50, 50)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Resize images to target size and normalize pixel values.\n",
    "    \n",
    "    Args:\n",
    "        images: List of numpy arrays representing images\n",
    "        target_size: Tuple of (height, width) for output images\n",
    "        \n",
    "    Returns:\n",
    "        Numpy array of processed images with shape (n_images, height, width, 3)\n",
    "    \"\"\"\n",
    "    processed_images = []\n",
    "    errors = 0\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        try:\n",
    "            # Convert numpy array to PIL Image\n",
    "            pil_img = Image.fromarray((img * 255).astype(np.uint8))\n",
    "            \n",
    "            # Resize image\n",
    "            resized_img = pil_img.resize(target_size, Image.Resampling.BILINEAR)\n",
    "            \n",
    "            # Convert back to numpy array and normalize\n",
    "            img_array = np.array(resized_img).astype(np.float32) / 255.0\n",
    "            \n",
    "            processed_images.append(img_array)\n",
    "            \n",
    "            # Print progress every 10000 images\n",
    "            if (i + 1) % 10000 == 0:\n",
    "                print(f\"Processed {i + 1}/{len(images)} images...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            if errors <= 5:  # Only show first 5 errors\n",
    "                print(f\"Error processing image {i}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if errors > 0:\n",
    "        print(f\"\\nTotal errors encountered: {errors}\")\n",
    "    \n",
    "    # Stack all processed images into a single array\n",
    "    X = np.stack(processed_images)\n",
    "    \n",
    "    print(f\"\\nFinal array shape: {X.shape}\")\n",
    "    print(f\"Value range: [{X.min():.3f}, {X.max():.3f}]\")\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Helper function to check memory usage\n",
    "def get_memory_usage(X: np.ndarray) -> str:\n",
    "    \"\"\"Calculate memory usage of numpy array in MB\"\"\"\n",
    "    return f\"{X.nbytes / (1024 * 1024):.2f} MB\"\n",
    "\n",
    "# Example usage\n",
    "def process_dataset(images, labels):\n",
    "    \"\"\"Process complete dataset with images and labels\"\"\"\n",
    "    print(\"Starting image processing...\")\n",
    "    X = resize_and_normalize_images(images)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    print(\"\\nDataset summary:\")\n",
    "    print(f\"Images shape: {X.shape}\")\n",
    "    print(f\"Labels shape: {y.shape}\")\n",
    "    print(f\"Memory usage: {get_memory_usage(X)}\")\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting image processing...\n",
      "Processed 10000/277524 images...\n",
      "Processed 20000/277524 images...\n",
      "Processed 30000/277524 images...\n",
      "Processed 40000/277524 images...\n",
      "Processed 50000/277524 images...\n",
      "Processed 60000/277524 images...\n",
      "Processed 70000/277524 images...\n",
      "Processed 80000/277524 images...\n",
      "Processed 90000/277524 images...\n",
      "Processed 100000/277524 images...\n",
      "Processed 110000/277524 images...\n",
      "Processed 120000/277524 images...\n",
      "Processed 130000/277524 images...\n",
      "Processed 140000/277524 images...\n",
      "Processed 150000/277524 images...\n",
      "Processed 160000/277524 images...\n",
      "Processed 170000/277524 images...\n",
      "Processed 180000/277524 images...\n",
      "Processed 190000/277524 images...\n",
      "Processed 200000/277524 images...\n",
      "Processed 210000/277524 images...\n",
      "Processed 220000/277524 images...\n",
      "Processed 230000/277524 images...\n",
      "Processed 240000/277524 images...\n",
      "Processed 250000/277524 images...\n",
      "Processed 260000/277524 images...\n",
      "Processed 270000/277524 images...\n"
     ]
    }
   ],
   "source": [
    "# Process your dataset\n",
    "X, y = process_dataset(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (277524, 50, 50, 3)\n",
      "y shape: (277524,)\n"
     ]
    }
   ],
   "source": [
    "print(f'X shape: {X.shape}')\n",
    "print(f'y shape: {y.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (222019, 50, 50, 3)\n",
      "Validation set shape: (27752, 50, 50, 3)\n",
      "Test set shape: (27753, 50, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "# First, split into training and temp (validation + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42)\n",
    "\n",
    "# Now, split the temp set equally into validation and test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=42)\n",
    "\n",
    "print(f'Training set shape: {X_train.shape}')\n",
    "print(f'Validation set shape: {X_val.shape}')\n",
    "print(f'Test set shape: {X_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened training set shape: (222019, 7500)\n"
     ]
    }
   ],
   "source": [
    "# Flatten the images\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "print(f'Flattened training set shape: {X_train_flat.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to be uncommented later coz it is our Knns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train_flat, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred_knn = knn.predict(X_val_flat)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"KNN Classification Report (Validation Set):\")\n",
    "print(classification_report(y_val, y_val_pred_knn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# training the model here \n",
    "gnb.fit(X_train_flat, y_train)\n",
    "\n",
    "# prediction on the validation set\n",
    "y_val_pred_gnb = gnb.predict(X_val_flat)\n",
    "\n",
    "# evaluation of the model\n",
    "print(\"Gaussian Naive Bayes Classification Report (Validation Set):\")\n",
    "print(classification_report(y_val, y_val_pred_gnb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS DO NOT WORK YET - PLEASE SKIP CELL\n",
    "# Instantiate the Logistic Regression classifier\n",
    "lr = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Train the model\n",
    "lr.fit(X_train_flat, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred_lr = lr.predict(X_val_flat)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Logistic Regression Classification Report (Validation Set):\")\n",
    "print(classification_report(y_val, y_val_pred_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESAMPLING OUR DATASET: Finxing the Imbalance Issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try 1 : Undersampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training set shape: Counter({0: 158990, 1: 63029})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Original class distribution in the training set\n",
    "print('Original training set shape:', Counter(y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Separate the indices of each class\n",
    "class_0_indices = np.where(y_train == 0)[0]\n",
    "class_1_indices = np.where(y_train == 1)[0]\n",
    "\n",
    "# Number of samples in the minority class\n",
    "n_class_1 = len(class_1_indices)\n",
    "\n",
    "# Randomly select indices from the majority class\n",
    "np.random.seed(42)  # For reproducibility\n",
    "class_0_selected_indices = np.random.choice(class_0_indices, size=n_class_1, replace=False)\n",
    "\n",
    "# Combine the selected indices\n",
    "undersampled_indices = np.concatenate([class_0_selected_indices, class_1_indices])\n",
    "\n",
    "# Create undersampled training data\n",
    "X_train_undersampled = X_train_flat[undersampled_indices]\n",
    "y_train_undersampled = y_train[undersampled_indices]\n",
    "\n",
    "# Shuffle the undersampled dataset\n",
    "from sklearn.utils import shuffle\n",
    "X_train_undersampled, y_train_undersampled = shuffle(X_train_undersampled, y_train_undersampled, random_state=42)\n",
    "\n",
    "# Check the new class distribution\n",
    "print('Undersampled training set shape:', Counter(y_train_undersampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the KNN classifier\n",
    "knn_undersampled = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the model\n",
    "knn_undersampled.fit(X_train_undersampled, y_train_undersampled)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred_knn_undersampled = knn_undersampled.predict(X_val_flat)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"KNN Classification Report after Undersampling (Validation Set):\")\n",
    "print(classification_report(y_val, y_val_pred_knn_undersampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Gaussian Naive Bayes classifier\n",
    "gnb_undersampled = GaussianNB()\n",
    "\n",
    "# Train the model\n",
    "gnb_undersampled.fit(X_train_undersampled, y_train_undersampled)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred_gnb_undersampled = gnb_undersampled.predict(X_val_flat)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Gaussian Naive Bayes Classification Report after Undersampling (Validation Set):\")\n",
    "print(classification_report(y_val, y_val_pred_gnb_undersampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS DO NOT WORK YET - PLEASE SKIP CELL\n",
    "\n",
    "# Instantiate the Logistic Regression classifier\n",
    "lr_undersampled = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Train the model\n",
    "lr_undersampled.fit(X_train_undersampled, y_train_undersampled)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred_lr_undersampled = lr_undersampled.predict(X_val_flat)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Logistic Regression Classification Report after Undersampling (Validation Set):\")\n",
    "print(classification_report(y_val, y_val_pred_lr_undersampled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try 2: SYNTHETIC MINORITY OVER SAMPLING TECHNIQUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information about SMOTE: https://medium.com/@corymaklin/synthetic-minority-over-sampling-technique-smote-7d419696b88c\n",
    "\n",
    "Synthetic minority over sampling technique: it creates new examples of the minority classes by generating data that is close to the existings points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All the code below hasn't been ran yet, (i have an error when running locally that I havent figured out yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: Counter({0: 158990, 1: 63029})\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Before SMOTE\n",
    "print('Original dataset shape:', Counter(y_train))\n",
    "\n",
    "# Apply SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = sm.fit_resample(X_train_flat, y_train)\n",
    "\n",
    "# After SMOTE\n",
    "print('Resampled dataset shape:', Counter(y_train_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred_knn_balanced = knn.predict(X_val_flat)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"KNN Classification Report after SMOTE (Validation Set):\")\n",
    "print(classification_report(y_val, y_val_pred_knn_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "# Retrain Gaussian Naive Bayes\n",
    "gnb.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred_gnb_balanced = gnb.predict(X_val_flat)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Gaussian Naive Bayes Classification Report after SMOTE (Validation Set):\")\n",
    "print(classification_report(y_val, y_val_pred_gnb_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain Logistic Regression\n",
    "lr.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred_lr_balanced = lr.predict(X_val_flat)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Logistic Regression Classification Report after SMOTE (Validation Set):\")\n",
    "print(classification_report(y_val, y_val_pred_lr_balanced))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
